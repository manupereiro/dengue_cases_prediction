#!/usr/bin/env python3
"""
Script para unificar todos los archivos dengue_processed_20xx_with_trends.csv
en un solo dataset consolidado para an√°lisis y machine learning

El script:
1. Busca todos los archivos dengue_processed_20xx_with_trends.csv
2. Los combina en un solo DataFrame
3. Ordena por a√±o, semana y departamento
4. Valida la consistencia de los datos
5. Guarda el resultado en un archivo unificado
"""

import pandas as pd
import numpy as np
import os
import glob
from collections import Counter

def find_trends_files():
    """
    Encuentra todos los archivos dengue_processed_*_with_trends.csv
    """
    pattern = "data/dengue_processed_*_with_trends.csv"
    files = glob.glob(pattern)
    
    # Extraer a√±os de los nombres de archivos
    files_with_years = []
    for file in files:
        try:
            filename = os.path.basename(file)
            # Extraer a√±o: dengue_processed_YYYY_with_trends.csv
            parts = filename.split('_')
            year = int(parts[2])
            files_with_years.append((year, file))
        except:
            print(f"‚ö†Ô∏è  No se pudo extraer a√±o de: {file}")
    
    # Ordenar por a√±o
    files_with_years.sort()
    
    return files_with_years

def load_and_validate_file(year, filepath):
    """
    Carga y valida un archivo individual
    """
    try:
        df = pd.read_csv(filepath)
        
        # Validaciones b√°sicas
        required_columns = [
            'year', 'week', 'departament_name', 'departament_id', 
            'province', 'trends_dengue', 'prec_2weeks', 
            'temp_2weeks_avg', 'humd_2weeks_avg', 'target_cases'
        ]
        
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            print(f"‚ö†Ô∏è  {year}: Faltan columnas: {missing_cols}")
        
        # Verificar que el a√±o coincide
        unique_years = df['year'].unique()
        if len(unique_years) != 1 or unique_years[0] != year:
            print(f"‚ö†Ô∏è  {year}: A√±os inconsistentes en archivo: {unique_years}")
        
        # Estad√≠sticas b√°sicas
        total_records = len(df)
        unique_weeks = len(df['week'].unique())
        unique_depts = len(df['departament_name'].unique())
        
        # Datos de trends
        trends_filled = df['trends_dengue'].notna().sum()
        trends_missing = df['trends_dengue'].isna().sum()
        
        print(f"‚úì {year}: {total_records} registros, {unique_weeks} semanas, {unique_depts} departamentos")
        print(f"    Trends: {trends_filled} completos, {trends_missing} faltantes")
        
        return df
        
    except Exception as e:
        print(f"‚ùå Error cargando {year}: {e}")
        return pd.DataFrame()

def analyze_combined_data(df_combined):
    """
    Analiza el dataset combinado y muestra estad√≠sticas
    """
    print(f"\n{'='*60}")
    print(f"AN√ÅLISIS DEL DATASET UNIFICADO")
    print(f"{'='*60}")
    
    # Estad√≠sticas generales
    total_records = len(df_combined)
    years_range = f"{df_combined['year'].min()}-{df_combined['year'].max()}"
    unique_years = sorted(df_combined['year'].unique())
    unique_depts = len(df_combined['departament_name'].unique())
    unique_provinces = df_combined['province'].unique()
    
    print(f"üìä RESUMEN GENERAL:")
    print(f"   Total registros: {total_records:,}")
    print(f"   A√±os: {years_range} ({len(unique_years)} a√±os)")
    print(f"   Departamentos √∫nicos: {unique_depts}")
    print(f"   Provincias: {list(unique_provinces)}")
    
    # Distribuci√≥n por a√±o
    print(f"\nüìÖ DISTRIBUCI√ìN POR A√ëO:")
    year_counts = df_combined['year'].value_counts().sort_index()
    for year, count in year_counts.items():
        print(f"   {year}: {count:,} registros")
    
    # Distribuci√≥n por provincia
    print(f"\nüó∫Ô∏è  DISTRIBUCI√ìN POR PROVINCIA:")
    province_counts = df_combined['province'].value_counts()
    for province, count in province_counts.items():
        print(f"   {province}: {count:,} registros")
    
    # An√°lisis de completitud de datos
    print(f"\nüîç COMPLETITUD DE DATOS:")
    for col in ['trends_dengue', 'prec_2weeks', 'temp_2weeks_avg', 'humd_2weeks_avg']:
        total = len(df_combined)
        complete = df_combined[col].notna().sum()
        percentage = (complete / total) * 100
        print(f"   {col}: {complete:,}/{total:,} ({percentage:.1f}%)")
    
    # An√°lisis de casos de dengue
    print(f"\nü¶ü AN√ÅLISIS DE CASOS DE DENGUE:")
    total_cases = df_combined['target_cases'].sum()
    max_cases_week = df_combined['target_cases'].max()
    avg_cases_week = df_combined['target_cases'].mean()
    
    print(f"   Total casos: {total_cases:,}")
    print(f"   M√°ximo semanal: {max_cases_week}")
    print(f"   Promedio semanal: {avg_cases_week:.2f}")
    
    # Top departamentos con m√°s casos
    dept_cases = df_combined.groupby('departament_name')['target_cases'].sum().sort_values(ascending=False)
    print(f"\nüèÜ TOP 10 DEPARTAMENTOS CON M√ÅS CASOS:")
    for i, (dept, cases) in enumerate(dept_cases.head(10).items(), 1):
        print(f"   {i:2d}. {dept}: {cases:,} casos")
    
    # An√°lisis temporal
    print(f"\nüìà AN√ÅLISIS TEMPORAL:")
    weekly_cases = df_combined.groupby(['year', 'week'])['target_cases'].sum()
    peak_week = weekly_cases.idxmax()
    peak_cases = weekly_cases.max()
    print(f"   Pico m√°ximo: A√±o {peak_week[0]}, Semana {peak_week[1]} ({peak_cases} casos)")
    
    # An√°lisis de Google Trends
    if not df_combined['trends_dengue'].isna().all():
        print(f"\nüîç AN√ÅLISIS GOOGLE TRENDS:")
        trends_stats = df_combined['trends_dengue'].describe()
        print(f"   M√≠nimo: {trends_stats['min']:.0f}")
        print(f"   M√°ximo: {trends_stats['max']:.0f}")
        print(f"   Promedio: {trends_stats['mean']:.1f}")
        print(f"   Mediana: {trends_stats['50%']:.1f}")

def save_unified_dataset(df_combined, output_filename="data/dengue_unified_dataset.csv"):
    """
    Guarda el dataset unificado con validaciones finales
    """
    print(f"\nüíæ GUARDANDO DATASET UNIFICADO...")
    
    try:
        # Ordenar datos
        df_sorted = df_combined.sort_values(['year', 'week', 'province', 'departament_name']).reset_index(drop=True)
        
        # Guardar archivo principal
        df_sorted.to_csv(output_filename, index=False, encoding='utf-8')
        file_size = os.path.getsize(output_filename) / (1024 * 1024)  # MB
        print(f"‚úì Dataset principal guardado: {output_filename}")
        print(f"   Tama√±o: {file_size:.2f} MB")
        print(f"   Registros: {len(df_sorted):,}")
        
        # Crear versi√≥n resumida (sin duplicados por departamento-semana)
        summary_filename = output_filename.replace('.csv', '_summary.csv')
        df_summary = df_sorted.groupby(['year', 'week', 'province']).agg({
            'trends_dengue': 'first',  # Tomar el primer valor (deber√≠a ser igual para todos)
            'prec_2weeks': 'mean',
            'temp_2weeks_avg': 'mean', 
            'humd_2weeks_avg': 'mean',
            'target_cases': 'sum',  # Sumar casos por provincia-semana
            'departament_name': 'count'  # Contar departamentos
        }).rename(columns={'departament_name': 'num_departments'}).reset_index()
        
        df_summary.to_csv(summary_filename, index=False, encoding='utf-8')
        print(f"‚úì Resumen por provincia-semana guardado: {summary_filename}")
        print(f"   Registros: {len(df_summary):,}")
        
        # Crear dataset solo para machine learning (sin NaN)
        ml_filename = output_filename.replace('.csv', '_ml_ready.csv')
        df_ml = df_sorted.dropna(subset=['trends_dengue', 'prec_2weeks', 'temp_2weeks_avg', 'humd_2weeks_avg'])
        
        if len(df_ml) > 0:
            df_ml.to_csv(ml_filename, index=False, encoding='utf-8')
            print(f"‚úì Dataset ML listo guardado: {ml_filename}")
            print(f"   Registros: {len(df_ml):,} (sin valores faltantes)")
        else:
            print(f"‚ö†Ô∏è  No se pudo crear dataset ML (todos los registros tienen valores faltantes)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error guardando dataset: {e}")
        return False

def main():
    """
    Funci√≥n principal
    """
    print("=== UNIFICACI√ìN DE DATASETS DE DENGUE ===")
    print("Combinando todos los archivos dengue_processed_*_with_trends.csv")
    
    # 1. Encontrar archivos
    files_with_years = find_trends_files()
    
    if not files_with_years:
        print("‚ùå No se encontraron archivos dengue_processed_*_with_trends.csv")
        return
    
    print(f"\nüìÅ Archivos encontrados: {len(files_with_years)}")
    for year, filepath in files_with_years:
        print(f"   {year}: {filepath}")
    
    # 2. Cargar y combinar archivos
    print(f"\nüîÑ CARGANDO Y VALIDANDO ARCHIVOS...")
    
    dataframes = []
    successful_loads = 0
    
    for year, filepath in files_with_years:
        df = load_and_validate_file(year, filepath)
        if not df.empty:
            dataframes.append(df)
            successful_loads += 1
        else:
            print(f"‚ùå Fallo cargando {year}")
    
    if not dataframes:
        print("‚ùå No se pudo cargar ning√∫n archivo exitosamente")
        return
    
    print(f"\n‚úì Archivos cargados exitosamente: {successful_loads}/{len(files_with_years)}")
    
    # 3. Combinar todos los DataFrames
    print(f"\nüîó COMBINANDO DATASETS...")
    df_combined = pd.concat(dataframes, ignore_index=True)
    
    # 4. Analizar datos combinados
    analyze_combined_data(df_combined)
    
    # 5. Guardar dataset unificado
    if save_unified_dataset(df_combined):
        print(f"\nüéâ UNIFICACI√ìN COMPLETADA EXITOSAMENTE")
    else:
        print(f"\n‚ùå ERROR EN LA UNIFICACI√ìN")
    
    print(f"\n{'='*60}")

if __name__ == "__main__":
    main() 